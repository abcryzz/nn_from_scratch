import numpy as np
import h5py

# GRADED FUNCTION: initialize_parameters

def initialize_parameters(n_x, n_h, n_y):
  # here we are initiatizing without any fancy_method to keep it simple
    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))
  
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters 



def linear_activation_forward(A_prev, W, b, activation):

    # most commonly used activation function relu and sigmoid
    if activation == "sigmoid":
        
        Z=np.dot(W,A_prev)+b 
        A=1/(1+np.exp(-Z))
       
       
    
    elif activation == "relu":
       
        Z=np.dot(W,A_prev)+b
        A=np.maximum(0,Z)
        
    linear_cache = (A_prev, W, b)
    activation_cache = Z
        
       
    cache = (linear_cache, activation_cache)

    return A, cache


def compute_cost(AL, Y):
  
    m = Y.shape[1]

    cost=-(1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))
     # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).
    cost = np.squeeze(cost)    
    
    return cost




def linear_activation_backward(dA, cache, activation):
 
    linear_cache, activation_cache = cache
    A_prev,w,b=linear_cache
    z=activation_cache
    
    if activation == "relu":
        dz=dA*(z>0)
        dA_prev=np.dot(w.T,dz)
        dW=(1/z.shape[1])*np.dot(dz,A_prev.T)
        db=(1/z.shape[1])*np.sum(dz,axis=1,keepdims=True)
      
        
    elif activation == "sigmoid":
        dz=dA*((1/(1+np.exp(-z)))*(1-1/(1+np.exp(-z))))
        dA_prev=np.dot(w.T,dz)
        dW=(1/z.shape[1])*np.dot(dz,A_prev.T)
        db=(1/z.shape[1])*np.sum(dz,axis=1,keepdims=True)

    return dA_prev, dW, db

    
def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):

    
    np.random.seed(1)
    grads = {}
    costs = []                              # to keep track of the cost
    m = X.shape[1]                           # number of examples
    (n_x, n_h, n_y) = layers_dims
    
   
    parameters = initialize_parameters(n_x,n_h,n_y)
    
    
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    # Loop (gradient descent)

    for i in range(0, num_iterations):

      
        A1, cache1 = linear_activation_forward(X,W1,b1,activation="relu")
        
        A2, cache2 = linear_activation_forward(A1,W2,b2,activation="sigmoid")
    
        cost =-(1/m)*np.sum(Y*np.log(A2)+(1-Y)*np.log(1-A2))
       
        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))
        
        
        dA1, dW2, db2 = linear_activation_backward(dA2,cache2,activation="sigmoid")
        dA0, dW1, db1 =linear_activation_backward(dA1,cache1,activation="relu")
        
        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2
        grads['dW1'] = dW1
        grads['db1'] = db1
        grads['dW2'] = dW2
        grads['db2'] = db2
        
        # Update parameters.
        W1-=learning_rate*dW1
        W2-=learning_rate*dW2
        b1-=learning_rate*db1
        b1-=learning_rate*db2

       

        # Retrieve W1, b1, W2, b2 from parameters
        W1 = parameters["W1"]
        b1 = parameters["b1"]
        W2 = parameters["W2"]
        b2 = parameters["b2"]
        cost=np.array(cost)
        
        if i % 100 == 0 or i == num_iterations:
            costs.append(cost)

    return parameters,costs


    
    
